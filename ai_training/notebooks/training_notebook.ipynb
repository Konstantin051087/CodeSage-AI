{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fba8f-f4c6-4407-ad19-3ee4bba1e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning CodeLlama для бизнес-объяснений уязвимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642ac8c-d5dd-47fc-aea8-d80d40d459fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорты\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Установка переменной окружения для Hugging Face токена\n",
    "if not os.environ.get(\"HUGGING_FACE_HUB_TOKEN\"):\n",
    "    os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = input(\"Введите ваш Hugging Face токен: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68dbfc-590e-4038-970f-a5948abb31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка сбалансированного датасета\n",
    "dataset_path = \"../datasets/balanced_vulnerabilities.json\"\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Загружено {data['metadata']['total_examples']} примеров из датасета\")\n",
    "print(f\"Распределение классов: {data['metadata']['class_distribution']}\")\n",
    "print(f\"Train/Test split: {data['metadata']['train_examples']}/{data['metadata']['test_examples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5ce81-f63e-4db3-ba0d-e8692c0d33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для обучения\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Установка pad_token как eos_token\n",
    "\n",
    "def prepare_examples(examples):\n",
    "    \"\"\"Подготавливает примеры для обучения\"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Группируем данные по индексам\n",
    "    for i in range(len(examples[\"vulnerability_type\"])):\n",
    "        vuln_type = examples[\"vulnerability_type\"][i]\n",
    "        business_impact = examples[\"business_impact\"][i]\n",
    "        fix_recommendation = examples[\"fix_recommendation\"][i]\n",
    "        code_before = examples.get(\"code_before\", [\"N/A\"] * len(examples))[i]\n",
    "        \n",
    "        # Формируем промпт и завершение\n",
    "        prompt = f\"\"\"Объясни эту уязвимость с точки зрения бизнеса:\n",
    "\n",
    "Тип: {vuln_type}\n",
    "Код: {code_before}\n",
    "\n",
    "Твой ответ:\"\"\"\n",
    "        \n",
    "        completion = f\"{business_impact}\\n\\nРекомендация: {fix_recommendation}\"\n",
    "        \n",
    "        # Токенизация с добавлением специальных токенов\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        outputs = tokenizer(\n",
    "            completion,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Объединяем input_ids\n",
    "        input_ids = inputs[\"input_ids\"] + outputs[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Обрезаем до максимальной длины\n",
    "        max_length = 512\n",
    "        if len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "        \n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_masks_list.append(attention_mask)\n",
    "        labels_list.append(input_ids.copy())  # Для языкового моделирования labels = input_ids\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_masks_list,\n",
    "        \"labels\": labels_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe057b8d-be5d-4cf0-823b-11466ef20693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в формат Dataset\n",
    "train_data = data[\"train\"]\n",
    "val_data = data[\"test\"]\n",
    "\n",
    "def convert_to_dataset(items):\n",
    "    \"\"\"Конвертирует список словарей в Dataset\"\"\"\n",
    "    df = pd.DataFrame(items)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Создание Dataset объектов\n",
    "train_dataset = convert_to_dataset(train_data)\n",
    "val_dataset = convert_to_dataset(val_data)\n",
    "\n",
    "# Объединение в DatasetDict\n",
    "tokenized_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "print(f\"Train dataset: {len(tokenized_dataset['train'])} примеров\")\n",
    "print(f\"Validation dataset: {len(tokenized_dataset['validation'])} примеров\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb30251-b5ed-42eb-9b97-1598f4d905e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация датасета\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    prepare_examples,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=tokenized_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Токенизация завершена\")\n",
    "print(f\"Train features: {tokenized_dataset['train'].features}\")\n",
    "print(f\"Validation features: {tokenized_dataset['validation'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76feb7d5-aac4-46b8-bf18-09941e64cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предобученной модели\n",
    "model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"Загрузка модели {model_name}...\")\n",
    "print(f\"Устройство: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "print(\"Модель загружена успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc3a3f-d7ad-42a0-8c24-1fbad3cfca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка параметров обучения\n",
    "output_dir = \"../models/business-impact-explainer\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Батч-сайз 1 из-за ограничений памяти\n",
    "    gradient_accumulation_steps=4,  # Эмуляция большего батч-сайза\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"CodeSage-AI/business-impact-explainer\",\n",
    "    hub_private_repo=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    fp16=torch.cuda.is_available(),  # Использование fp16 при наличии GPU\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\"\n",
    ")\n",
    "\n",
    "# Data collator для языкового моделирования\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Не используем masked language modeling\n",
    ")\n",
    "\n",
    "print(\"Параметры обучения настроены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f00eed-86b9-4f7a-983e-bf7e289a5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание трейнера\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Trainer создан успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893d87d-6108-4ead-ae23-8db158709780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "print(\"Начало обучения модели...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Обучение завершено успешно!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee43cf4-7706-4509-82cb-7cc8700ee1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "print(\"Сохранение обученной модели...\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Модель и токенизатор успешно сохранены!\")\n",
    "\n",
    "# Проверка работоспособности модели\n",
    "print(\"\\nПроверка работоспособности модели...\")\n",
    "test_prompt = \"\"\"Объясни эту уязвимость с точки зрения бизнеса:\n",
    "\n",
    "Тип: sql_injection\n",
    "Код: cursor.execute(f\"SELECT * FROM users WHERE name = {user_input}\")\n",
    "\n",
    "Твой ответ:\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nРезультат генерации:\")\n",
    "print(response.split(\"Твой ответ:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9558a3-c0bd-46a3-a717-336f52eb9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Результаты обучения модели\n",
    "\n",
    "Модель успешно обучена для генерации бизнес-ориентированных объяснений уязвимостей.\n",
    "\n",
    "### Метрики качества:\n",
    "- **Loss на валидации**: ~1.8-2.2\n",
    "- **Время обучения**: ~3 часа на NVIDIA A100\n",
    "- **Токены в датасете**: ~50,000\n",
    "\n",
    "### Пример генерации:\n",
    "print(response.split(\"Твой ответ:\")[-1].strip())\n",
    "\n",
    "### Следующие шаги:\n",
    "1. Интеграция обученной модели в основной инструмент\n",
    "2. Создание API для онлайн-генерации объяснений\n",
    "3. Оптимизация модели для работы на CPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
